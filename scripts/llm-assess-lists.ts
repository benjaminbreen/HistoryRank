/**
 * LLM-Based List Quality Assessment
 *
 * Uses Claude Haiku 3.5 to provide qualitative assessment of generated lists.
 * Complements the automated metrics with "smell test" evaluation:
 * - Hallucination detection (fake/non-existent figures)
 * - Ranking coherence (do rankings make sense?)
 * - Historical accuracy (are contributions correct?)
 * - Diversity evaluation
 *
 * Samples strategically: top 25, middle 50, bottom 25 entries
 */

import fs from 'node:fs';
import path from 'node:path';

// Types
type ListEntry = {
  rank: number;
  name: string;
  primary_contribution: string;
};

type LLMAssessment = {
  scores: {
    ranking_coherence: number;
    historical_accuracy: number;
    no_hallucinations: number;
    contribution_quality: number;
    list_coherence: number;
  };
  overall_score: number;
  suspicious_entries: string[];
  glaring_omissions: string[];
  ranking_anomalies: { name: string; rank: number; issue: string }[];
  brief_assessment: string;
  subjective_impression?: string;
};

type AssessmentReport = {
  file: string;
  model: string;
  timestamp: string;
  assessment: LLMAssessment;
  sampled_entries: {
    top: number;
    middle: number;
    bottom: number;
  };
};

// Load env
function loadEnvFile(fileName: string) {
  const envPath = path.join(process.cwd(), fileName);
  if (!fs.existsSync(envPath)) return;
  const content = fs.readFileSync(envPath, 'utf8');
  for (const line of content.split('\n')) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) continue;
    const eqIndex = trimmed.indexOf('=');
    if (eqIndex === -1) continue;
    const key = trimmed.slice(0, eqIndex).trim();
    const value = trimmed.slice(eqIndex + 1).trim();
    if (!process.env[key]) {
      process.env[key] = value;
    }
  }
}

loadEnvFile('.env.local');

const OPENROUTER_API_KEY = process.env.OPENROUTER_API_KEY;
const HAIKU_MODEL = 'anthropic/claude-3.5-haiku';
const RAW_DIR = path.join(process.cwd(), 'data', 'raw');
const REPORTS_DIR = path.join(process.cwd(), 'data', 'llm-assessments');

// Ensure reports directory exists
if (!fs.existsSync(REPORTS_DIR)) {
  fs.mkdirSync(REPORTS_DIR, { recursive: true });
}

/**
 * Parse a list file and return entries
 */
function parseListFile(filePath: string): ListEntry[] {
  const content = fs.readFileSync(filePath, 'utf8');
  const start = content.indexOf('[');
  const end = content.lastIndexOf(']');
  if (start === -1 || end === -1) {
    throw new Error('No JSON array found in file');
  }
  const jsonStr = content.slice(start, end + 1);
  return JSON.parse(jsonStr) as ListEntry[];
}

/**
 * Extract model name from filename
 */
function extractModelName(filename: string): string {
  const match = filename.match(/^(.+?) LIST \d+/i);
  return match ? match[1].trim() : 'Unknown';
}

/**
 * Sample entries strategically: top 25, middle 50, bottom 25
 */
function sampleEntries(entries: ListEntry[]): {
  top: ListEntry[];
  middle: ListEntry[];
  bottom: ListEntry[];
} {
  const sorted = [...entries].sort((a, b) => a.rank - b.rank);

  return {
    top: sorted.slice(0, 25),
    middle: sorted.slice(475, 525),
    bottom: sorted.slice(-25),
  };
}

/**
 * Format entries for the prompt
 */
function formatEntriesForPrompt(entries: ListEntry[]): string {
  return entries
    .map(e => `${e.rank}. ${e.name} - ${e.primary_contribution}`)
    .join('\n');
}

/**
 * Build the assessment prompt
 */
function buildPrompt(sample: { top: ListEntry[]; middle: ListEntry[]; bottom: ListEntry[] }): string {
  return `You are evaluating a ranked list of the 1000 most historically influential people, generated by an AI model. You will see samples from the top, middle, and bottom of the list.

Evaluate the following criteria on a 1-10 scale:

1. **Ranking Coherence**: Do the rankings make sense? Is #1 more historically influential than #500? Is #500 more influential than #990? Is there a logical progression based on actual historical impact?
2. **Historical Accuracy**: Are the people real historical figures? Are their contributions described accurately and specifically?
3. **No Hallucinations**: Are all entries verifiably real historical figures? Flag any you cannot verify, suspect are fake, or seem far too obscure for a top-1000 list.
4. **Contribution Quality**: Are the "primary_contribution" descriptions accurate, specific, and meaningful (not vague or generic)?
5. **List Coherence**: Does the list feel like a unified, thoughtful ranking or does it fall apart/become random/repetitive in places?

Also identify (keep lists SHORT, max 3 items each):
- Any **suspicious entries** (potentially fake, too obscure, or anachronistic)
- Any **glaring omissions** you'd expect in any reasonable top-1000 historical influence list
- Any **ranking anomalies** (someone ranked absurdly high or low)

Finally, provide a **subjective_impression**: one honest sentence - your gut reaction. What stands out? What feels off? Is this a list made by someone who understands history, or does it feel broken/random/lazy somewhere? Be blunt.

Respond with ONLY valid JSON. Keep arrays short (max 3 items). Format:
{
  "scores": {
    "ranking_coherence": <1-10>,
    "historical_accuracy": <1-10>,
    "no_hallucinations": <1-10>,
    "contribution_quality": <1-10>,
    "list_coherence": <1-10>
  },
  "overall_score": <1-10>,
  "suspicious_entries": ["name1", "name2"],
  "glaring_omissions": ["name1", "name2"],
  "ranking_anomalies": [{"name": "...", "rank": 123, "issue": "..."}],
  "brief_assessment": "<2-3 sentences summarizing the list quality>",
  "subjective_impression": "<one candid sentence - your gut reaction>"
}

=== TOP 25 (Ranks 1-25) ===
${formatEntriesForPrompt(sample.top)}

=== MIDDLE 50 (Ranks 475-525) ===
${formatEntriesForPrompt(sample.middle)}

=== BOTTOM 25 (Ranks 976-1000) ===
${formatEntriesForPrompt(sample.bottom)}`;
}

/**
 * Call Claude Haiku via OpenRouter
 */
async function callHaiku(prompt: string): Promise<LLMAssessment> {
  if (!OPENROUTER_API_KEY) {
    throw new Error('OPENROUTER_API_KEY not set');
  }

  const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
      'Content-Type': 'application/json',
      'HTTP-Referer': 'https://historyrank.org',
      'X-Title': 'HistoryRank LLM Assessment',
    },
    body: JSON.stringify({
      model: HAIKU_MODEL,
      messages: [
        {
          role: 'user',
          content: prompt,
        },
      ],
      max_tokens: 4000,
      temperature: 0.2,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`OpenRouter API error: ${response.status} - ${errorText}`);
  }

  const data = await response.json() as {
    choices: { message: { content: string } }[];
  };

  const content = data.choices[0]?.message?.content;
  if (!content) {
    throw new Error('No content in API response');
  }

  // Extract JSON from response - handle common LLM JSON issues
  let jsonStr = content;

  // Try to find JSON object
  const jsonMatch = content.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    jsonStr = jsonMatch[0];
  }

  // Common fixes for LLM JSON output issues
  jsonStr = jsonStr
    // Remove trailing commas before } or ]
    .replace(/,\s*([}\]])/g, '$1')
    // Fix unquoted property names (rare but happens)
    .replace(/([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:/g, '$1"$2":')
    // Fix single quotes to double quotes (but not apostrophes in words)
    .replace(/"([^"]*?)'/g, '"$1"')
    .replace(/'([^']*?)"/g, '"$1"')
    // Remove control characters
    .replace(/[\x00-\x1F\x7F]/g, ' ')
    // Fix truncated JSON - close open structures
    .replace(/,\s*$/, '')
    .replace(/,\s*"\s*$/, '')
    .replace(/:\s*$/, ': null');

  // Count brackets to fix truncation
  const openBraces = (jsonStr.match(/\{/g) || []).length;
  const closeBraces = (jsonStr.match(/\}/g) || []).length;
  const openBrackets = (jsonStr.match(/\[/g) || []).length;
  const closeBrackets = (jsonStr.match(/\]/g) || []).length;

  // Add missing closing brackets/braces
  jsonStr += ']'.repeat(Math.max(0, openBrackets - closeBrackets));
  jsonStr += '}'.repeat(Math.max(0, openBraces - closeBraces));

  try {
    return JSON.parse(jsonStr) as LLMAssessment;
  } catch (parseError) {
    // Log the problematic JSON for debugging
    console.error(`   JSON parse error. Raw response:\n${content.slice(0, 500)}`);
    throw new Error(`JSON parse failed: ${parseError instanceof Error ? parseError.message : parseError}`);
  }
}

/**
 * Get all list files grouped by model
 */
function getListsByModel(): Map<string, string[]> {
  const files = fs.readdirSync(RAW_DIR).filter(f => f.endsWith('.txt') && f.includes('LIST'));
  const byModel = new Map<string, string[]>();

  for (const file of files) {
    const model = extractModelName(file);
    const existing = byModel.get(model) || [];
    existing.push(file);
    byModel.set(model, existing);
  }

  return byModel;
}

/**
 * Select one random list per model
 */
function selectRandomListPerModel(byModel: Map<string, string[]>): Map<string, string> {
  const selected = new Map<string, string>();

  for (const [model, files] of byModel) {
    const randomIndex = Math.floor(Math.random() * files.length);
    selected.set(model, files[randomIndex]);
  }

  return selected;
}

/**
 * Assess a single list
 */
async function assessList(filename: string): Promise<AssessmentReport> {
  const filePath = path.join(RAW_DIR, filename);
  const model = extractModelName(filename);

  console.log(`\nüìã Assessing: ${filename}`);
  console.log(`   Model: ${model}`);

  const entries = parseListFile(filePath);
  const sample = sampleEntries(entries);

  console.log(`   Sampled: ${sample.top.length} top, ${sample.middle.length} middle, ${sample.bottom.length} bottom`);

  const prompt = buildPrompt(sample);
  console.log(`   Calling Haiku...`);

  const assessment = await callHaiku(prompt);

  console.log(`   ‚úì Overall score: ${assessment.overall_score}/10`);
  if (assessment.suspicious_entries.length > 0) {
    console.log(`   ‚ö†Ô∏è  Suspicious entries: ${assessment.suspicious_entries.slice(0, 3).join(', ')}`);
  }

  return {
    file: filename,
    model,
    timestamp: new Date().toISOString(),
    assessment,
    sampled_entries: {
      top: sample.top.length,
      middle: sample.middle.length,
      bottom: sample.bottom.length,
    },
  };
}

/**
 * Main function
 */
async function main() {
  const args = process.argv.slice(2);

  // Check for specific file flag
  const fileArg = args.find(a => a.startsWith('--file='));
  const allFlag = args.includes('--all');

  let filesToAssess: string[] = [];

  if (fileArg) {
    // Assess specific file
    const filename = fileArg.slice('--file='.length);
    filesToAssess = [filename];
  } else if (allFlag) {
    // Assess one random list per model
    const byModel = getListsByModel();
    const selected = selectRandomListPerModel(byModel);
    filesToAssess = Array.from(selected.values());
    console.log(`\nüé≤ Selected ${filesToAssess.length} lists (one random per model)`);
  } else {
    console.log(`
LLM-Based List Quality Assessment
==================================

Usage:
  npx tsx scripts/llm-assess-lists.ts --all              Assess one random list per model
  npx tsx scripts/llm-assess-lists.ts --file="..."       Assess a specific file

Examples:
  npx tsx scripts/llm-assess-lists.ts --all
  npx tsx scripts/llm-assess-lists.ts --file="Claude Opus 4.5 LIST 1 (January 12, 2025).txt"
`);
    return;
  }

  const reports: AssessmentReport[] = [];

  for (const file of filesToAssess) {
    try {
      const report = await assessList(file);
      reports.push(report);

      // Save individual report
      const reportFilename = file.replace(/\.txt$/, '.llm-assessment.json');
      const reportPath = path.join(REPORTS_DIR, reportFilename);
      fs.writeFileSync(reportPath, JSON.stringify(report, null, 2));

      // Rate limit - be nice to API
      await new Promise(resolve => setTimeout(resolve, 1000));
    } catch (err) {
      console.error(`   ‚úó Error assessing ${file}:`, err instanceof Error ? err.message : err);
    }
  }

  // Generate summary
  if (reports.length > 0) {
    console.log(`\n${'='.repeat(60)}`);
    console.log('SUMMARY');
    console.log('='.repeat(60));

    // Sort by overall score
    const sorted = [...reports].sort((a, b) =>
      b.assessment.overall_score - a.assessment.overall_score
    );

    console.log('\nModel Rankings (by LLM Assessment):');
    console.log('-'.repeat(50));

    for (const report of sorted) {
      const s = report.assessment.scores;
      const suspicious = report.assessment.suspicious_entries.length;
      const anomalies = report.assessment.ranking_anomalies.length;

      console.log(`${report.assessment.overall_score.toString().padStart(2)}/10  ${report.model}`);
      console.log(`      Ranking: ${s.ranking_coherence}, Accuracy: ${s.historical_accuracy}, Hallucinations: ${s.no_hallucinations}`);
      console.log(`      Contributions: ${s.contribution_quality}, List Coherence: ${s.list_coherence}`);
      if (suspicious > 0 || anomalies > 0) {
        console.log(`      ‚ö†Ô∏è  ${suspicious} suspicious, ${anomalies} anomalies`);
      }
      if (report.assessment.subjective_impression) {
        console.log(`      üí≠ "${report.assessment.subjective_impression}"`);
      }
      console.log();
    }

    // Save summary
    const summaryPath = path.join(REPORTS_DIR, 'summary.json');
    fs.writeFileSync(summaryPath, JSON.stringify({
      timestamp: new Date().toISOString(),
      total_assessed: reports.length,
      rankings: sorted.map(r => ({
        model: r.model,
        file: r.file,
        overall_score: r.assessment.overall_score,
        scores: r.assessment.scores,
        suspicious_count: r.assessment.suspicious_entries.length,
        anomaly_count: r.assessment.ranking_anomalies.length,
        subjective_impression: r.assessment.subjective_impression || null,
      })),
    }, null, 2));

    console.log(`\nüìÅ Reports saved to: ${REPORTS_DIR}`);
    console.log(`   Summary: ${summaryPath}`);
  }
}

main().catch(console.error);
